{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimbul/Desktop/personal/deep-learning/llm-training/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import GPTModel, GPTConfig\n",
    "import torch \n",
    "import tiktoken \n",
    "import torch.nn.functional  as F \n",
    "from transformers import GPT2LMHeadModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Functional structure \n",
    "# 2. Parameters \n",
    "# 3. Data(Large Scale Data Optimization) - Stocastic Optimization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nn.Linear(in_features=5, out_features=2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.2945,  0.0140, -0.2965,  0.3601, -0.3736],\n",
       "                      [ 0.3168,  0.0893,  0.4162,  0.1639,  0.0308]]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([-0.4434, -0.2760, -0.1864, -0.0997,  0.2051] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_llama = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B-Instruct', token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(hf_gpt, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_state_dict = hf_gpt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in hf_state_dict:\n",
    "    print(key, hf_state_dict[key].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chanadana():\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Chandana learning deep nets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Chanadana())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj  =DataLoader(B=4, T=8)\n",
    "x,y = data_obj.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model import GPTModel\n",
    "gpt_m = GPTModel(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  =torch.zeros(768)\n",
    "n = 100 \n",
    "for i in range(100):\n",
    "    x = x + torch.randn(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimbul/Desktop/personal/deep-learning/llm-training/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "/Users/dimbul/Desktop/personal/deep-learning/llm-training/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Our Implementation \n",
    "gpt_conf = GPTConfig()\n",
    "gpt_model, hf_weights = GPTModel(gpt_conf).load_pretrained_weights()\n",
    "gpt_state = gpt_model.state_dict()\n",
    "\n",
    "from transformers import AutoConfig\n",
    "## Hugging Face Model \n",
    "gpt_hf = GPT2LMHeadModel.from_pretrained('gpt2', output_hidden_states=True, output_attentions=True)\n",
    "gpt_hf_state =gpt_model.state_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens,dtype=torch.long)\n",
    "x = tokens.unsqueeze(0).repeat(5,1)\n",
    "x_ind = x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "logits, a,b,c = gpt_hf.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, not a science. I'm a language designer. I want to write, I want to think. I want\n",
      "> Hello, I'm a language model, I use an English sentence structure, I like words over sentences.\n",
      "\n",
      "\"That's OK I'll look\n",
      "> Hello, I'm a language model, not just another language.\" This isn't a \"language model?\" It's an idea. So far, what\n",
      "> Hello, I'm a language model, not a programming model. I'm not a theoretical computer model - you read that right - because my ideas are\n",
      "> Hello, I'm a language model, I teach myself.\n",
      "\n",
      "I want to know more about how languages work and why they could be used.\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens,dtype=torch.long)\n",
    "x = tokens.unsqueeze(0).repeat(5,1)\n",
    "x_ind = x[0,:]\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "max_length = 30\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, tok_emb, pos_emb, out_dict = gpt_model.forward(x)\n",
    "\n",
    "        probs = F.softmax(logits[:,-1,:], dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs,50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs,1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        x = torch.concat((x,xcol),dim=1)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens,dtype=torch.long)\n",
    "x = tokens.unsqueeze(0).repeat(5,1)\n",
    "x_ind = x[0,:]\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "max_length = 30\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        out  = gpt_hf.forward(x)\n",
    "        logits = out['logits']\n",
    "\n",
    "        probs = F.softmax(logits[:,-1,:], dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs,50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs,1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        x = torch.concat((x,xcol),dim=1)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_hf = gpt_hf(x)\n",
    "out_model, tok_emb, pos_emb, out_dict = gpt_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam = (tok_emb + pos_emb)[0,:,:].view(1,8,gpt_conf.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#our model\n",
    "m_0_ln_1 = gpt_model.transformer.h[0].ln_1.forward(exam)\n",
    "m_0_qkv  = gpt_model.transformer.h[0].attn.c_attn.forward(m_0_ln_1)\n",
    "m_0_att_c_attn  = gpt_model.transformer.h[0].attn.c_attn.forward(m_0_ln_1)\n",
    "m_0_att  = gpt_model.transformer.h[0].attn.forward(m_0_ln_1)\n",
    "\n",
    "#hugging model\n",
    "\n",
    "h_0_ln_1 = gpt_hf.transformer.h[0].ln_1.forward(exam)\n",
    "h_0_qkv  = gpt_hf.transformer.h[0].attn.c_attn.forward(h_0_ln_1)\n",
    "h_0_att  = gpt_hf.transformer.h[0].attn.forward(h_0_ln_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "\n",
    "h_q,h_k,h_v = h_0_qkv.split(768, dim=2)\n",
    "\n",
    "#####\n",
    "h_q = gpt_hf.transformer.h[0].attn._split_heads(h_q, 12, 768//12)\n",
    "h_k = gpt_hf.transformer.h[0].attn._split_heads(h_k, 12, 768//12)\n",
    "h_v = gpt_hf.transformer.h[0].attn._split_heads(h_v, 12, 768//12)\n",
    "\n",
    "#####\n",
    "h_att, h_att_weights = gpt_hf.transformer.h[0].attn._attn( h_q, h_k, h_v)\n",
    "h_att_merged = gpt_hf.transformer.h[0].attn._merge_heads(h_att, 12, 768//12)\n",
    "h_att_c_proj  = gpt_hf.transformer.h[0].attn.c_proj(h_att_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_att_c_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "q,k,v = m_0_qkv.split(gpt_conf.n_embd, dim=2)\n",
    "####\n",
    "(B,T, d_model) = q.size()\n",
    "d  = d_model//gpt_conf.n_head\n",
    "h  = gpt_conf.n_head\n",
    "####\n",
    "\n",
    "\n",
    "q = q.view(B,T,h,d).transpose(1,2)\n",
    "k = k.view(B,T,h,d).transpose(1,2)\n",
    "v = v.view(B,T,h,d).transpose(1,2)\n",
    "\n",
    "#####\n",
    "qk_t =(q @ k.transpose(-1,-2) ) * (1/math.sqrt(d)) \n",
    "mask = torch.tril(torch.ones(100,100).view(1,1,100,100))\n",
    "score = qk_t.masked_fill(mask[:,:,:T,:T]==0, float('-inf'))\n",
    "att_weights = F.softmax(score,dim=-1)\n",
    "att = att_weights @ v \n",
    "att_merged = att.transpose(1,2).contiguous().view(B,T,d_model)\n",
    "att_c_proj = gpt_model.transformer.h[0].attn.c_proj(att_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_proj_w, c_proj_b = gpt_model.transformer.h[0].attn.c_proj.state_dict()['weight'], gpt_model.transformer.h[0].attn.c_proj.state_dict()['bias'] \n",
    "h_c_proj_w, h_c_proj_b = gpt_hf.transformer.h[0].attn.c_proj.state_dict()['weight'], gpt_hf.transformer.h[0].attn.c_proj.state_dict()['bias'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hf.transformer.h[0].attn.c_proj(att_merged[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.transformer.h[0].attn.c_proj(att_merged[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hf.transformer.h[0].attn.c_proj(att_merged[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_proj_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_c_proj_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_merged[0,0,:] @ c_proj_w.T + c_proj_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_merged[0,0,:] @ h_c_proj_w + h_c_proj_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_c_proj[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_att_c_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_merged == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_hf.transformer.h[0].attn.c_proj.state_dict()['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.transformer.h[0].attn.c_proj.state_dict()['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.transformer.h[0].attn.c_proj.state_dict()['weight'] == gpt_hf.transformer.h[0].attn.c_proj.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_merged == h_att_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = out.transpose(1,2).contiguous().view(B,T, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.transformer.h[0].attn.c_proj(out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_0_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_0_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(m_0_ln_1==h_0_ln_1, m_0_att_c_attn==h_0_att_c_attn)\n",
    "print(m_0_ln_1==h_0_ln_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( m_0_att_c_attn==h_0_att_c_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m_0_att_c_proj==h_0_att_c_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0_att_c_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_0_att_c_attn==h_0_att_c_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_0_att_c_attn.split(gpt_conf.n_embd,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0_att_c_attn.split(gpt_conf.n_embd,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m_embd = gpt_state['transformer.wte.weight'][x_ind,:]\n",
    "x_m_pos  = gpt_state['transformer.wpe.weight'][[0,1,2,3,4,5,6,7],:]\n",
    "\n",
    "x_hf_embd = gpt_hf_state['transformer.wte.weight'][x_ind,:]\n",
    "x_hf_pos  = gpt_hf_state['transformer.wpe.weight'][[0,1,2,3,4,5,6,7],:]\n",
    "\n",
    "print()\n",
    "print(\"hf :: \")\n",
    "print()\n",
    "print(\"#########\")\n",
    "print()\n",
    "print(\"hf :: \")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m_embd + x_m_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hf_embd+x_hf_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_hf.hidden_states[0][0,:,:] == (x_m_embd + x_m_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_hf.hidden_states[1][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_hf.hidden_states[1][1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict[0][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict[0][1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
